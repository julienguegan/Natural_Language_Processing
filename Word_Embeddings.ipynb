{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:18.731609Z",
     "start_time": "2020-03-10T18:30:16.158707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contrôle de version\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "# Packages nécessaires\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "# A la première utilisation de nltk, télécharger les données nécessaires\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (<span style=\"color:red\">TODO</span> pour signaler du contenu manquant)\n",
    "\n",
    "##  Word Embeddings : Représentations distribuées via l'hypothèse distributionelle\n",
    "\n",
    "**But**: On va chercher à obtenir des représentations denses (comme vecteurs de nombres réels) de mots (et éventuellement de phrases). Ces représentations ont vocation à être distribuées: ce sont des représentations non-locales. On représente un objet comme une combinaison de *features*, par opposition à l'attribution d'un symbole dédié: voir le travail fondateur d'entre autres, Geoffrey Hinton, sur le sujet: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
    "\n",
    "Le terme de représentation *distribuées* est très général, mais correspond à que l'on cherche à obtenir. L'enjeu est donc de pouvoir construire, automatiquement, de telles représentations.\n",
    "\n",
    "**Idée sous-jacente**: Elle est basée sur l'hypothèse distributionelle: les informations contextuelles suffisent à obtenir une représentation viable d'objets linguistiques.\n",
    " - *“For a large class of cases [...] the  meaning  of a word is  its  use in the  language.”* Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
    " - *“You shall know a word by the company it keeps”*, Firth (\"A synopsis of linguistic theory 1930-1955.\" - 1957)\n",
    "\n",
    "Ainsi, on peut caractériser un mot par les mots qui l'accompagnent, via des comptes de co-occurences. Deux mots ayant un sens similaire auront une distribution contextuelle similaire et auront donc plus de chance d'apparaître dans des contextes similaires. Cette hypothèse peut servir de justification à l'application de statistiques à la sémantique (extraction d'information, analyse sémantique). Elle permet aussi une certaine forme de généralisation: on peut supposer que les informations que l'on a à propos d'un mot se généraliseront aux mots à la distribution similaire. \n",
    "\n",
    "**Motivation**: On cherche à obtenir des représentations distribuées pour pouvoir, de manière **efficace**:\n",
    "- Directement réaliser une analyse sémantique de surface.\n",
    "- S'en servir comme source d'informations pour d'autres modèles et applications liées au language, notamment pour l'analyse de sentiments. \n",
    "\n",
    "\n",
    "**Terminologie**: Attention à ne pas confondre l'idée de représentation *distribuée* et *distributionelle*. Le second indique en général (pour les mots) que la représentation a été obtenue strictement à partir de comptes de co-occurences, alors qu'on pourra utiliser des informations supplémentaires (labels de documents, tags de partie du discours, ...) pour construire des représentations distribuées. \n",
    "Les modèles qui permettent de construire ces représentations denses, sous forme de vecteurs, sont souvent appellés *vector spaces models*. On appelle aussi régulièrement ces représentations des *word embeddings*, car les mots sont embarqués (*embedded*) dans un espace vectoriel. En Français, on rencontre souvent le terme *plongements de mots* ou *plongements lexicaux*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenir une représentation: comptes d'occurences et de co-occurences\n",
    "\n",
    "Selon le type de corpus dont on dispose, on pourra obtenir différents types d'informations distributionelles. Si l'on a accès à une collection de documents, on pourra ainsi choisir de compter le nombre d'occurence de chaque mot dans chacun des documents, pour obtenir une matrice $mots \\times documents$: c'est sur ce principe qu'est construit **Tf-Idf** (vu au TP précédent). On va maintenant s'intéresser à un cas plus général: on dispose d'une grande quantité de données sous forme de texte, et on cherche à obtenir des représentations de mots sous forme de vecteurs de taille réduite, sans avoir besoin d'un découpage en documents ou catégories. \n",
    "\n",
    "Supposons qu'on dispose d'un corpus contenant $T$ mots différents. On va construire une matrice $\\mathbf{M}$ de taille $T \\times T$ qui contiendra le nombre de co-occurences entre les mots. Il y aura différents facteurs à considérer lors de la construction de cette matrice: \n",
    "- Comment définir le 'contexte' d'un mot, qui permettra de dire que les termes qu'il contient co-occurent avec ce mot ? On pourra choisir d'utiliser différentes échelles: le document, la phrase, le groupe nominal, ou tout simplement une fenêtre de $k$ mots, selon les informations que l'on cherche à capturer.\n",
    "*Encore une fois, si par exemple notre corpus est divisé en $D$ documents, on pourra même s'intéresser aux liens distributionnels entre mots et documents: chacun de ces $D$ documents agira comme un \"contexte\", et on construit une matrice d'occurences $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$.* \n",
    "- Comment quantifier l'importance des comptes ? Par exemple, on pourra donner un poids décroissant à une co-occurence selon la distance entre les deux mots concernés ($\\frac{1}{d+1}$ pour une séparation par $d$ mots).\n",
    "- Faut-il garder tous les mots qui apparaissent dans le corpus ? En général, non. On verra que pour les grands corpus, le nombre de mots différents $T$ est énorme. Deuxièmement, même si le nombre de mots est raisonnable, on ne possèdera que très peu d'information distributionelle sur les mots les plus rares, et la représentation obtenue sera à priori de mauvaise qualité. Il faudra se poser la question de comment filtrer ces mots, et de comment traiter les mots qu'on choisit de ne pas représenter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procédure\n",
    "\n",
    "Pour construire la matrice, on va dans un premier temps recueillir la liste des mots différents (ou le *vocabulaire* $V$) qui apparaissent dans le corpus sous forme de dictionaire {mots -> index}\n",
    "Puis, pour chaque terme $w$ du corpus,\n",
    "- On récupère l'index $i$ correspondant à l'aide de $V$\n",
    "- Pour chaque terme $w'$ du contexte de $w$, \n",
    "  + On récupère l'index $j$ correspondant à l'aide de $V$\n",
    "  + On incrémente $\\mathbf{M}_{i,j}$ par le poids correspondant, ou par $1$. \n",
    "  \n",
    "La procédure est très proche de celle qu'on a suivi au TP précédent, excepté qu'il faut maintenant compter les mots suivant leur apparition \n",
    "  \n",
    "#### Exemple\n",
    "\n",
    "On considère le corpus suivant: \n",
    "\n",
    "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
    "\n",
    "On choisit de définir le contexte d'un mot comme la phrase à laquelle il appartient, et de ne pas utiliser de poids. \n",
    "On obtient la matrice suivante: \n",
    "\n",
    "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
    "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
    "| I             | 0 |      7 |    2 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
    "| the           | 7 |      2 |    6 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
    "| down          | 2 |      6 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
    "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
    "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
    "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
    "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
    "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
    "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:18.742238Z",
     "start_time": "2020-03-10T18:30:18.734373Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir un Vocabulaire:\n",
    "\n",
    "Cette fois, on va implémenter séparément une fonction retournant le vocabulaire. Il faudra ici pouvoir contrôler sa taille, que ce soit en indiquant un nombre maximum de mots, ou un nombre minimum d'occurences pour qu'on prenne en compte les mots. On ajoute, à la fin, un mot \"inconnu\" qui remplacera tous les mots qui n'apparaissent pas dans notre vocabulaire 'limité'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:18.885739Z",
     "start_time": "2020-03-10T18:30:18.746090Z"
    }
   },
   "outputs": [],
   "source": [
    "def vocabulary(corpus, count_threshold=1, voc_threshold=0):\n",
    "    \"\"\"    \n",
    "    Function using word counts to build a vocabulary\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
    "        voc_threshold (int): maximum size of the vocabulary - 0 (default) indicates there is no max\n",
    "    Returns:\n",
    "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
    "                                 values: indexes corresponding to each word sorted by frequency \n",
    "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
    "                                             values: word counts in the corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    word_counts = {}\n",
    "    for text in corpus:\n",
    "        text_clean = clean_and_tokenize(text)\n",
    "        for word in text_clean:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "                \n",
    "    # tri le dictionnaire selon la valeur de count\n",
    "    word_counts_sorted = {word: count for word, count in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)} \n",
    "                \n",
    "    filtered_word_counts = {}     \n",
    "    for word, count in word_counts_sorted.items():\n",
    "        if count >= count_threshold:\n",
    "            if voc_threshold == 0:\n",
    "                filtered_word_counts[word] = count\n",
    "            else:\n",
    "                if len(filtered_word_counts) <= voc_threshold:\n",
    "                     filtered_word_counts[word] = count\n",
    "\n",
    "    \n",
    "    vocabulary = dict([ (word,i) for i,word in enumerate(filtered_word_counts) ]) # create dictionnary \"word\":counts\n",
    "    vocabulary['UNK'] = len(vocabulary)\n",
    "    vocabulary_word_counts = filtered_word_counts\n",
    "    vocabulary_word_counts['UNK'] = 0\n",
    "    \n",
    "    return vocabulary, vocabulary_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:19.132229Z",
     "start_time": "2020-03-10T18:30:18.887608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
      "{'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
      "{'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
      "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example for testing:\n",
    "\n",
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "\n",
    "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
    "\n",
    "voc, counts = vocabulary(corpus)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir les co-occurences:\n",
    "\n",
    "La fonction prend en entrée le corpus (une liste de strings, correspondant aux documents, ou phrases) et un vocabulaire, ainsi que la taille de la fenêtre de contexte. On pourra aussi implémenter la solution la plus simple - que le contexte d'un mot soit le reste du document duquel il provient. \n",
    "Enfin, on pourra implémenter la possibilité de faire décroitre linéairement l'importance d'un mot du contexte à mesure qu'on s'éloigne du mot d'origine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:19.444903Z",
     "start_time": "2020-03-10T18:30:19.135988Z"
    }
   },
   "outputs": [],
   "source": [
    "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        vocabulary (dictionary): words to use in the matrix\n",
    "        window (int): size of the context window; when 0, the context is the whole sentence\n",
    "        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n",
    "    Returns:\n",
    "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
    "    \"\"\" \n",
    "    l = len(vocabulary)\n",
    "    M = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Obtenir la phrase:\n",
    "        sent = clean_and_tokenize(sent)\n",
    "        # Obtenir les indexs de la phrase grace au vocabulaire: \n",
    "        sent_idx = [vocabulary[word] if word in vocabulary else vocabulary['UNK'] for word in sent] \n",
    "        # Parcourir les indexs de la phrase et ajouter 1 / dist(i,j) à M[i,j] si les mots d'index i et j apparaissent dans la même fenêtre. \n",
    "        for i, idx_i in enumerate(sent_idx):\n",
    "            # On vérifie que le mot est reconnu par le vocabulaire:\n",
    "            if idx_i is not None: # | idx_i > -1 :                \n",
    "                # Si on considère un contexte limité:\n",
    "                if window > 0:\n",
    "                    # On crée une liste qui contient les indexs de la fenêtre à gauche de l'index courant 'idx_i'\n",
    "                    l_ctx_idx = sent_idx[max(0,i - window):i]\n",
    "                # Si on considère que le contexte est la phrase entière:\n",
    "                else:\n",
    "                    #l_ctx_idx = np.delete(sent_idx,i)\n",
    "                    l_ctx_idx = sent_idx[0:i]\n",
    "                # On parcourt cette liste et on update M[i,j]:    \n",
    "                for j, idx_j in enumerate(l_ctx_idx):\n",
    "                    # ... en s'assurant que le mot correspondant à 'idx_j' est reconnu par le vocabulaire\n",
    "                    if idx_j is not None: # | idx_j > -1:\n",
    "                        # Calcul du poids:\n",
    "                        if distance_weighting:\n",
    "                            dist = i-j\n",
    "                            weight = 1.0/(dist+1)\n",
    "                        else:\n",
    "                            weight = 1.0\n",
    "                        M[idx_i, idx_j] += weight * 1.0\n",
    "                        M[idx_j, idx_i] += weight * 1.0\n",
    "\n",
    "    return M  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T16:57:58.554260Z",
     "start_time": "2020-03-02T16:57:47.955Z"
    }
   },
   "source": [
    "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
    "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
    "| I             | 0 |      7 |    2 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
    "| the           | 7 |      2 |    6 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
    "| down          | 2 |      6 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
    "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
    "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
    "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
    "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
    "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
    "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |\n",
    "\n",
    "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:19.890938Z",
     "start_time": "2020-03-10T18:30:19.465402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "co-occurence matrix:\n",
      "\n",
      "[[2 7 6 3 3 2 2 1 1 0]\n",
      " [7 2 6 2 2 3 3 1 1 0]\n",
      " [6 6 0 2 2 2 2 1 1 0]\n",
      " [3 2 2 0 1 1 0 0 0 0]\n",
      " [3 2 2 1 0 0 0 1 0 0]\n",
      " [2 3 2 1 0 0 1 0 0 0]\n",
      " [2 3 2 0 0 1 0 0 1 0]\n",
      " [1 1 1 0 1 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nco-occurence matrix:\\n')\n",
    "print(co_occurence_matrix(corpus, voc, 0, False).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application à un vrai jeu de données\n",
    "\n",
    "On va chercher à obtenir ces comptes pour les données **imdb**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:20.416999Z",
     "start_time": "2020-03-10T18:30:19.905575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "filenames_neg = sorted(glob(os.path.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(os.path.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etude rapide des données\n",
    "\n",
    "On voudrait ici, avant de procéder, avoir une idée de ce que contiennent ces critiques de films. On va donc obtenir le vocabulaire (entier) et représenter les fréquences des mots, dans l'ordre (attention, il faudra utiliser une échelle logarithmique): on devrait retrouver la loi de Zipf. Cela nous permettra d'avoir une idée de la taille du vocabulaire qu'on pourra choisir : il s'agit de réaliser un compromis entre les ressources nécessaires (taille des objets en mémoire) et quantité d'informations qu'on peut en tirer (les mots rares peuvent apporter beaucoup d'informations, mais il est difficile d'en apprendre de bonnes représentations,car ils sont rares !)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:30:34.060519Z",
     "start_time": "2020-03-10T18:30:20.418952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41682\n",
      "Part of the corpus by taking the 10000 most frequent words : 1228199 / 1305272 = 94 %\n",
      "Part of the corpus by taking the 5000 most frequent words  : 1158320 / 1305272 = 88 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We would like to display the curve of word frequencies given their rank (index) in the vocabulary\n",
    "vocab, word_counts = vocabulary(texts)\n",
    "N_total = sum(list(word_counts.values())) # total number of word in corpus\n",
    "frequencies = np.array(list(word_counts.values()))/N_total # word frequencies in [0,1]\n",
    "\n",
    "# We can for example use the function plt.scatter()\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Word counts vs rank',fontsize=20)\n",
    "plt.plot(list(vocab.values()),frequencies,label='corpus frequencies') # word_counts est trié par ordre décroissant\n",
    "plt.yscale('log')\n",
    "\n",
    "# We would like to know how much of the data is represented by the 'k' most frequent words\n",
    "print('Vocabulary size: %i' % len(vocab))\n",
    "\n",
    "cpt_10000 = 0\n",
    "cpt_5000 = 0\n",
    "for i in range(10000):\n",
    "    for word, index in vocab.items():  \n",
    "        if index == i:\n",
    "            w = word\n",
    "            break\n",
    "    cpt_10000 = cpt_10000 + word_counts[w]\n",
    "    if i < 5000:\n",
    "        cpt_5000 = cpt_5000 + word_counts[w]\n",
    "        \n",
    "part = int(100*cpt_10000/N_total) \n",
    "print('Part of the corpus by taking the 10000 most frequent words :', cpt_10000, '/', N_total, '=', part,'%')\n",
    "plt.plot([10000,10000],[0,0.1],'--',color='grey')\n",
    "plt.text(11000,1e-6,'x='+str(10000),c='grey',fontsize=10)\n",
    "plt.text(11000,0.05,str(part)+'% of the corpus',c='grey',fontsize=15)\n",
    "\n",
    "part = int(100*cpt_5000/N_total) \n",
    "print('Part of the corpus by taking the 5000 most frequent words  :', cpt_5000, '/', N_total, '=', part,'%')\n",
    "plt.plot([5000,5000],[0,0.1],'--',color='grey')\n",
    "plt.text(5500,1e-6,'x='+str(5000),c='grey',fontsize=10)\n",
    "plt.text(5500,0.01,str(part)+'% of the corpus',c='grey',fontsize=15)\n",
    "\n",
    "# loi de zipf\n",
    "z = []\n",
    "L = len(vocab)\n",
    "s = 1.07\n",
    "S = sum(1/(np.arange(1,L,dtype=np.int64))**s)\n",
    "for k in np.arange(1,L)+1:\n",
    "    z.append((1/k**s)/S)\n",
    "plt.plot(z,'r',alpha=0.4,linewidth=5,label='loi de Zipf')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "<u>__Résultat de l'analyse__</u> : on peut se contenter d'un vocabulaire de 10000, voire 5000 mots et toujours capter 90% du corpus. C'est important, car cela va déterminer la taille des objets que l'on va manipuler.\n",
    "    \n",
    "/!\\ Les mots rares sont ceux porteur de sens ?\n",
    "</font>\n",
    "\n",
    "On va maintenant recréer la matrice de co-occurence avec différents paramètres. Cela peut-être long: si cela pose problème, travaillez avec un vocabulaire plus réduit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:31:41.082283Z",
     "start_time": "2020-03-10T18:30:34.062469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5002, 5002)\n",
      "(5002, 5002)\n"
     ]
    }
   ],
   "source": [
    "vocab_5k, word_counts_5k = vocabulary(texts, 0, 5000)\n",
    "M5dist = co_occurence_matrix(texts, vocab_5k, window=5, distance_weighting=True)\n",
    "M20    = co_occurence_matrix(texts, vocab_5k, window=20, distance_weighting=False)\n",
    "print(M5dist.shape)\n",
    "print(M20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:46.015686Z",
     "start_time": "2020-03-10T18:31:41.084121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5002, 5002)\n",
      "(5002, 5002)\n"
     ]
    }
   ],
   "source": [
    "vocab_10k, word_counts_10k = vocabulary(texts, 0, 10000)\n",
    "M5dist_20 = co_occurence_matrix(texts, vocab_5k, window=5, distance_weighting=True)\n",
    "M20_20    = co_occurence_matrix(texts, vocab_5k, window=20, distance_weighting=False)\n",
    "print(M5dist_20.shape)\n",
    "print(M20_20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:46.029317Z",
     "start_time": "2020-03-10T18:32:46.017689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835\n",
      "[1.96868539 3.21092962 0.50866539 ... 0.         0.         1.79794703]\n",
      "[ 820.  452.  322. ...    0.    0. 1304.]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_5k['cinema'])\n",
    "print(M5dist[429])\n",
    "print(M20[429])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison de vecteurs\n",
    "\n",
    "On peut se servir de ces vecteurs de très grande taille pour une analyse sémantique très basique: par exemple, en cherchant les plus proches voisins d'un mot. Cependant, il faudra faire attention aux distances qu'on utilise, liées à certaines métriques (Euclidiennes, Cosine) ou éventuellement d'autres liées à l'appartenance aux ensembles (Matching, Jaccard). La normalisation des vecteurs peut aussi jouer un rôle. Dans tous les cas, il faut bien faire attention à ne pas sur-interprêter ce type de résultats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:46.235406Z",
     "start_time": "2020-03-10T18:32:46.034373Z"
    }
   },
   "outputs": [],
   "source": [
    "def euclidean(u, v):\n",
    "    return np.linalg.norm(u-v)\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / np.sqrt(u.dot(u)) if np.any(u) != 0 else 0. * u\n",
    "\n",
    "def cosine(u, v):\n",
    "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def get_neighbors(distance, voc, co_oc, mot, k=10):\n",
    "    inv_voc = {id: w for w, id in voc.items()}\n",
    "    neigh = NearestNeighbors(k, algorithm='brute', metric=distance)\n",
    "    neigh.fit(co_oc) \n",
    "    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n",
    "    neighbors = []\n",
    "    for s in ind:\n",
    "        for i in s[1:]:\n",
    "            neighbors.append(inv_voc[i])\n",
    "    return neighbors\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "*Note que ici on utilise la distance cosine (qui est positive) et non la cosine similarity. On peut avoir des mots avec des cosine similarity de signe différents avec la même cosine distance et donc qui se retrouvent considérés comme voisins sémantiques.*\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:49.309656Z",
     "start_time": "2020-03-10T18:32:46.242086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULAIRE 5000 - voisin de 'good'\n",
      "    [5 mots] - [pondéré]: \n",
      "         distance 'euclidean': ['bad', 'plot', 'very', 'just', 'comedy', 'little', 'man', 'life', 'big']\n",
      "         distance 'cosine'   : ['bad', 'very', 'great', 'plot', 'funny', 'its', 'not', 'movie', 'little']\n",
      "    [20 mots] - [non pondéré] : \n",
      "         distance 'euclidean': ['story', 'can', 'also', 'much', 'will', 'time', 'character', 'we', 'films']\n",
      "         distance 'cosine'   : ['bad', 'very', 'great', 'not', 'really', 'quite', 'it', 'fun', 'completely']\n",
      "\n",
      "VOCABULAIRE 10000 - voisin de 'good'\n",
      "    [5 mots] - [pondéré]: \n",
      "         distance 'euclidean': ['bad', 'plot', 'very', 'just', 'comedy', 'little', 'man', 'life', 'big']\n",
      "         distance 'cosine'   : ['bad', 'very', 'great', 'plot', 'funny', 'its', 'not', 'movie', 'little']\n",
      "    [20 mots] - [non pondéré] : \n",
      "         distance 'euclidean': ['story', 'can', 'also', 'much', 'will', 'time', 'character', 'we', 'films']\n",
      "         distance 'cosine'   : ['bad', 'very', 'great', 'not', 'really', 'quite', 'it', 'fun', 'completely']\n"
     ]
    }
   ],
   "source": [
    "print('VOCABULAIRE 5000 - voisin de \\'good\\'')\n",
    "\n",
    "print('    [5 mots] - [pondéré]: ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, M5dist, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, M5dist, 'good'))\n",
    "print('    [20 mots] - [non pondéré] : ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, M20, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, M20, 'good'))\n",
    "\n",
    "print('\\nVOCABULAIRE 10000 - voisin de \\'good\\'')\n",
    "\n",
    "print('    [5 mots] - [pondéré]: ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_10k, M5dist_20, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_10k, M5dist_20, 'good'))\n",
    "print('    [20 mots] - [non pondéré] : ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_10k, M20_20, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_10k, M20_20, 'good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "On remarque que dans les 2 types de contextes la distance cosine est semble plus pertinente pour comparer la sémantique des mots que la distance euclidienne, en effet la distance cosine est utile dans les espaces positifs de grandes dimensions et prend en compte l'orientation des vecteurs plus que leur magnitude (contrairement à la distance euclidienne).\n",
    "Malgré tout ce n'est pas parfait puisque même avec la distance cosine une partie des mots voisins n'est pas proche sémantiquement du mot good.\n",
    "    \n",
    "Utiliser 10000 mots ne semble pas apporter de meilleurs résultats au modèle\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthodes de pondération des matrices\n",
    "\n",
    "**Motivation**: On ne se base pour l'instant que sur la fréquence (ou au mieux, une pondération de la fréquence) pour construire ces représentations. Comme on peut s'en douter, _la fréquence seule n'est pas suffisante pour capturer des informations sémantiques intéressantes_. On peut l'illustrer avec le phénomène des mots très fréquents qui apparaissent dans de nombreux contextes très différents, ou de mots qui apparaissent très souvent ensemble sans avoir nécessairement de lien sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalisation**: Très simple; il s'agit d'_annuler l'influence de la magnitude des comptes sur la représentation._\n",
    "\n",
    "$$\\mathbf{m_{normalized}} = \\left[ \n",
    "   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "   \\ldots\n",
    "   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n",
    "\\right]$$\n",
    " \n",
    "**Pointwise Mutual Information**: Il s'agit d'_évaluer à quel point la co-occurence des deux termes est inattendue_. En effet, cette mesure correspond au ratio de la probabilité jointe des deux mots et du produit de leur probabilités individuelles:\n",
    "$$\n",
    "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
    "$$\n",
    "La probabilité jointe des deux mots correspond au nombre de fois ou on les observe ensemble, divisé par le nombre total de co-occurences du corpus: \n",
    "$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "La probabilité individuelle d'un mot correspond simplement à sa fréquence, que l'on peut calculer en comptant toutes les co-occurences ou ce mot apparaît:\n",
    "$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
    "Ainsi,\n",
    "$$ \n",
    "\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n",
    "$$\n",
    "On calcule ainsi le décalage entre l'observation que l'on a fait dans notre corpus et la fréquence d'apparition de ces termes si on les considère indépendant - c'est à dire qu'on suppose que _leur co-occurence est une coïncidence_.\n",
    "\n",
    "Le principal problème avec cette mesure est qu'_elle n'est pas adaptée au cas où l'on observe aucune co-occurence_. Puisque la PMI est censée renvoyer une quantité positive si l'on observe plus de co-occurences que prévu, et négative si l'on en observe moins, on ne peut pas choisir de remplacer $\\log(0)$ par $0$. Une solution couramment utilisée est d'utiliser la **Positive PMI**, qui fixe toutes les valeurs négatives à $0$.\n",
    " \n",
    " $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n",
    " \\begin{cases}\n",
    " \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n",
    " 0 & \\textrm{otherwise}\n",
    " \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:49.320494Z",
     "start_time": "2020-03-10T18:32:49.311609Z"
    }
   },
   "outputs": [],
   "source": [
    "def pmi(co_oc, positive=True):\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    sum_tot = sum_vec.sum()\n",
    "    with np.errstate(divide='ignore'):\n",
    "        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n",
    "    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n",
    "    if positive:\n",
    "        pmi[pmi < 0] = 0.0\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:55.860576Z",
     "start_time": "2020-03-10T18:32:49.322344Z"
    }
   },
   "outputs": [],
   "source": [
    "PPMI5dist  = pmi(M5dist)\n",
    "PPMI20     = pmi(M20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:57.788177Z",
     "start_time": "2020-03-10T18:32:55.862694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________ PPMI ________________________\n",
      "\n",
      "VOCABULAIRE 5000 - voisin de 'good'\n",
      "\n",
      "    [5 mots] - [pondéré]: \n",
      "         distance 'euclidean': ['portions', 'text', 'UNK', 'the', 'redman', 'warned', 'clint', 'potent', 'postman']\n",
      "         distance 'cosine'   : ['bad', 'but', 'for', 'its', 'and', 'comedy', 'it', 'very', 'funny']\n",
      "\n",
      "    [20 mots] - [non pondéré] : \n",
      "         distance 'euclidean': ['but', 'is', 'and', 'a', 'it', 'this', 'the', 'that', 'in']\n",
      "         distance 'cosine'   : ['but', 'pretty', 'actors', 'acting', 'bad', 'better', 'very', 'movie', 'performances']\n"
     ]
    }
   ],
   "source": [
    "print(\"________________________ PPMI ________________________\\n\")  \n",
    "\n",
    "print('VOCABULAIRE 5000 - voisin de \\'good\\'\\n')\n",
    "\n",
    "print('    [5 mots] - [pondéré]: ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, PPMI5dist, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, PPMI5dist, 'good'))\n",
    "print(\"\")\n",
    "print('    [20 mots] - [non pondéré] : ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, PPMI20, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, PPMI20, 'good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "En regardant surtout la distance cosine, les résultats en appliquant la PMI pour le mot 'good' ne semble pas meilleurs que ceux avec la simple matrice de co-occurence.\n",
    "\n",
    "Cependant, les résultats avec un contexte de 20 mots (sans poids) ne sont pas non plus dénués de sens puisque que pour un corpus de texte qui est tiré de critique de films, on imagine facilement que le mot 'good' soit souvent associé avec les mots 'pretty', 'actors', 'acting', 'better', 'very', 'movie', 'performances' ou encore 'but' (pour des critiques, il est logique de trouver des phrases comme 'he is good but ...')\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**: Comme on l'a déjà vu, il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF). \n",
    "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice $termes \\times documents$. Ainsi, pour une matrice $\\mathbf{X}$ de $n$ termes et $d$ documents: \n",
    "\n",
    " $$\\text{TF}(X, i, j) = \\frac{X_{i,j}}{\\sum_{i=1}^{t} X_{i,j}} $$\n",
    " \n",
    " $$\\text{IDF}(X, i) = \\log\\left(\\frac{d}{|\\{j : X_{i,j} > 0\\}|}\\right)$$\n",
    " \n",
    " $$\\text{TF-IDF}(X, i, j) = \\text{TF}(X, i, j) \\cdot \\text{IDF}(X, i)$$\n",
    "\n",
    "\n",
    "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. Ainsi, l'appliquer aux co-occurences des mots les plus fréquents n'est à priori pas optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:57.800865Z",
     "start_time": "2020-03-10T18:32:57.790521Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(co_oc):\n",
    "    \"\"\"\n",
    "    Inverse document frequencies applied to our co_oc matrices\n",
    "    \"\"\"\n",
    "    # IDF\n",
    "    d = float(co_oc.shape[1])\n",
    "    in_doc = co_oc.astype(bool).sum(axis=1)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        idfs = np.log(d / in_doc)\n",
    "    idfs[np.isinf(idfs)] = 0.0  # log(0) = 0\n",
    "    # TF\n",
    "    sum_vec = co_oc.sum(axis=0)\n",
    "    tfs = co_oc / sum_vec\n",
    "    return (tfs.T * idfs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:32:59.667004Z",
     "start_time": "2020-03-10T18:32:57.803794Z"
    }
   },
   "outputs": [],
   "source": [
    "TFIDF5dist = tfidf(M5dist)\n",
    "TFIDF20    = tfidf(M20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:33:01.062662Z",
     "start_time": "2020-03-10T18:32:59.669976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________ TF-IDF ________________________\n",
      "\n",
      "VOCABULAIRE 5000 - voisin de 'good'\n",
      "\n",
      "    [5 mots] - [pondéré]: \n",
      "         distance 'euclidean': ['that', 'as', 'with', 'is', 'in', 'but', 'equivalent', 'charm', 'masterful']\n",
      "         distance 'cosine'   : ['suddenly', 'will', 'hunting', 'a', 'comedy', 'in', 'is', 'charm', 'of']\n",
      "\n",
      "    [20 mots] - [non pondéré] : \n",
      "         distance 'euclidean': ['movie', 'even', 'or', 'just', 'only', 'some', 'than', 'more', 'what']\n",
      "         distance 'cosine'   : ['but', 'this', 'it', 'for', 'movie', 'not', 'film', 'one', 'its']\n"
     ]
    }
   ],
   "source": [
    "print(\"________________________ TF-IDF ________________________\\n\")  \n",
    "\n",
    "print('VOCABULAIRE 5000 - voisin de \\'good\\'\\n')\n",
    "\n",
    "print('    [5 mots] - [pondéré]: ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, TFIDF5dist, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, TFIDF5dist, 'good'))\n",
    "print(\"\")\n",
    "print('    [20 mots] - [non pondéré] : ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, TFIDF20, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, TFIDF20, 'good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:59:24.610969Z",
     "start_time": "2020-03-05T20:59:24.594845Z"
    }
   },
   "source": [
    "<font color=\"blue\"> \n",
    "Les résultats en appliquant la TF-IDF ne semble pas plus concluant, ils sont même pire que les précédents. On pouvait s'y attendre puisque cette méthode est plutôt adaptée aux matrices peu denses, puisqu'elle pénalise les termes qui apparaissent dans une grande partie des documents. Ainsi, l'appliquer aux co-occurences des mots les plus fréquents ne donne pas des résultats correct.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de co-occurences : Réduction de dimension\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Il s'agit non seulement de réduire la taille de données (ainsi, on traitera des vecteurs de dimension réduite, plutôt que de travailler avec des vecteurs de la taille du vocabulaire) mais aussi de mettre en évidence des relations de plus haut niveau entre les mots: en réduisant leurs représentations aux dimensions qui *les plus importantes* des données, on se retrouve à *généraliser* certaines propriétés entre les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réduction de dimension via SVD \n",
    "\n",
    "Une matrice est une transformation linéaire: y appliquer une SVD, c'est décomposer notre transformation linéaire en un produit de transformations linéaires de différents types. Il va s'agir d'effectuer un changement de base, et de replacer nos données dans un espace ou chacune des coordonnées sont inchangées par la transformation effectuée. Ainsi, on décompose la matrice $\\mathbf{M}$ en trois matrices:\n",
    "\n",
    "$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n",
    "\n",
    "Les matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ ont les propriétés suivantes:\n",
    "- $\\mathbf{U}$ et $\\mathbf{V}$ sont des matrices orthogonales ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ et $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). Elles contiennent les vecteurs propres à gauche et à droite de $\\mathbf{M}$.\n",
    "- $\\mathbf{\\lambda}$ est une matrice diagonale: attention, elle n'est pas forcément carrée. Les coefficients de la diagonale sont les valeurs propres de $\\mathbf{M}$.\n",
    "\n",
    "Ainsi, les dimensions *les plus importantes* correspondent aux plus grandes valeurs propres. Réduire nos données à une dimension $k$ correspond à ne garder que les vecteurs correspondant aux $k$ premières valeurs propres - et cela revient à prendre les $k$ premiers vecteurs de la matrice $U$. \n",
    "On utilise ici ```TruncatedSVD``` du package ```scikit-learn```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:33:10.290112Z",
     "start_time": "2020-03-10T18:33:01.069493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5002, 300)\n",
      "(5002, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "\n",
    "SVD5dist = svd.fit_transform(M5dist)\n",
    "print(SVD5dist.shape)\n",
    "\n",
    "SVD20 = svd.fit_transform(M20)\n",
    "print(SVD20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:33:10.854307Z",
     "start_time": "2020-03-10T18:33:10.296982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________ SVD ________________________\n",
      "\n",
      "VOCABULAIRE 5000 - voisin de 'good'\n",
      "\n",
      "    [5 mots] - [pondéré]: \n",
      "         distance 'euclidean': ['bad', 'plot', 'very', 'just', 'comedy', 'little', 'man', 'life', 'big']\n",
      "         distance 'cosine'   : ['bad', 'very', 'funny', 'fine', 'great', 'plot', 'pace', 'shock', 'basically']\n",
      "\n",
      "    [20 mots] - [non pondéré] : \n",
      "         distance 'euclidean': ['story', 'can', 'also', 'much', 'will', 'time', 'character', 'we', 'films']\n",
      "         distance 'cosine'   : ['bad', 'very', 'great', 'really', 'quite', 'not', 'fun', 'serious', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(\"________________________ SVD ________________________\\n\")  \n",
    "\n",
    "print('VOCABULAIRE 5000 - voisin de \\'good\\'\\n')\n",
    "\n",
    "print('    [5 mots] - [pondéré]: ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, SVD5dist, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, SVD5dist, 'good'))\n",
    "print(\"\")\n",
    "print('    [20 mots] - [non pondéré] : ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, SVD20, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, SVD20, 'good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Lorsque l'on applique cette méthode à la matrice des comptes $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$, on obtient la méthode appellée **Latent Semantic Analysis**, pour la détection de composantes latentes (sémantiques) permettant de regrouper les documents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation en deux dimensions\n",
    "\n",
    "On va maintenant utiliser **l'analyse en composantes principales** (PCA) pour visualiser nos données en 2 dimensions.  Cela revient à appliquer la SVD à la matrice de covariance des données, pour que les directions principales soient indépendantes les unes des autres et maximisent la variance des données.\n",
    "On utilise la classe ```PCA``` du package ```scikit-learn```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:33:12.298675Z",
     "start_time": "2020-03-10T18:33:10.856177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(M5dist)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'oscar', 'award']\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "x_words = [Emb[ind,0] for ind in ind_words]\n",
    "y_words = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "On visualise la proximité des mots movies et film mais il reste tout de même un paquet de mot dont on ne distingue pas grand chose.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:33:14.644982Z",
     "start_time": "2020-03-10T18:33:12.301603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "Emb = pca.fit_transform(Norm5)\n",
    "\n",
    "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
    "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
    "         'film', 'movie', 'oscar', 'award']\n",
    "ind_words = [vocab_5k[w] for w in words]\n",
    "x_words   = [Emb[ind,0] for ind in ind_words]\n",
    "y_words   = [Emb[ind,1] for ind in ind_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_words, y_words)\n",
    "\n",
    "for i, w in enumerate(words):\n",
    "    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "En normalisant les distances obtenues, on parvient à mieux visualiser les résultats. On note que bad-good-great sont proches l'un de l'autres ainsi que movie-scene-film, ce qui est plutôt satisfaisant. Il est difficile de tirer de réelles conclusions pour les autres mots\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenir une représentation: algorithmes couramment utilisés\n",
    "\n",
    "L'idée, ici, est de définir un ensemble de représentations ${w_{i}}_{i=1}^{V}$, de dimension prédéfinie $d$ (ici, on travaillera avec $d = 300$), pour tous les mots $i$ du vocabulaire $V$ - puis **d'entraîner** ces représentations pour qu'elles correspondent à ce que l'on souhaite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove\n",
    "\n",
    "L'objectif défini par Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) est d'apprendre des vecteurs $w_{i}$ et $w_{k}$ de façon à ce que leur produit scalaire correspondent au logarithme de leur **Pointwise Mutual Information**: \n",
    "\n",
    "\n",
    "$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n",
    "\n",
    "\n",
    "Dans l'article, l'obtention de cet objectif est minutieusement justifié par un raisonnement sur les opérations que l'on veut effectuer avec ces vecteurs et les propriétés qu'ils devraient avoir - notamment, une symétrie entre les lignes et les colonnes (voir l'article pour plus de détails).  \n",
    "L'objectif final obtenu est le suivant, où $M$ est la matrice de co-occurences:\n",
    "\n",
    "\n",
    "$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n",
    "  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n",
    "  \n",
    " \n",
    "Ici, $f$ est une fonction de *mise à l'échelle* qui permet de diminuer l'importance des comptes de co-occurences les plus fréquents: \n",
    "\n",
    "\n",
    "$$f(x) \n",
    "\\begin{cases}\n",
    "(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n",
    "1 & \\textrm{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "En général, on choisit $\\alpha=0.75$ et $x_{\\max} = 100$, même si ces paramètres peuvent nécessiter un changement selon les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant utilise l'API de gensim pour récupérer des représentations pré-entrainées (Il est normal que le chargement soit long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:33.961966Z",
     "start_time": "2020-03-10T18:33:14.647910Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut extraire la matrice des embeddings ainsi, et vérifier sa taille:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:33.977581Z",
     "start_time": "2020-03-10T18:36:33.964999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 300)\n"
     ]
    }
   ],
   "source": [
    "loaded_glove_embeddings = loaded_glove_model.vectors\n",
    "print(loaded_glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit donc qu'il y a $400.000$ mots représentés, et que les embeddings sont de dimension $300$. On définit une fonction qui nous renvoie, à partir du modèle chargé, le vocabulaire et la matrice des embeddings suivant les structures que l'on a utilisé auparavant. On ajoute, ici encore, un mot inconnu ```'UNK'``` au cas où se trouve dans nos données des mots qui ne font pas parti des $400.000$ mots représentés ici. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:34.108494Z",
     "start_time": "2020-03-10T18:36:33.980510Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_glove_voc_and_embeddings(glove_model):\n",
    "    voc = {word : index for word, index in enumerate(glove_model.index2word)}\n",
    "    voc['UNK'] = len(voc)\n",
    "    embeddings = glove_model.vectors\n",
    "    return voc, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:34.509506Z",
     "start_time": "2020-03-10T18:36:34.115349Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de comparer 'à jeu égal' les représentations chargées ici et celles que l'on a produite, il faudrait utiliser le même vocabulaire. Dans ce but, je réutilise le code qui suit pour créer un vocabulaire de $5000$ mots à partir des données exactement comme hier, et j'ajoute à la fin une fonction qui renvoie la matrices des représentations chargées avec Glove pour ces $5000$ mots seulement, dans le bon ordre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:34.529100Z",
     "start_time": "2020-03-10T18:36:34.514386Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_glove_adapted_embeddings(glove_model, input_voc):\n",
    "    keys = {i: glove_model.vocab.get(w, None) for w, i in input_voc.items()}\n",
    "    index_dict = {i: key.index for i, key in keys.items() if key is not None}\n",
    "    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n",
    "    for i, ind in index_dict.items():\n",
    "        embeddings[i] = glove_model.vectors[ind]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:34.677491Z",
     "start_time": "2020-03-10T18:36:34.533904Z"
    }
   },
   "outputs": [],
   "source": [
    "GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction prend donc en entrée le modèle chargé à l'aide de l'API Gensim, ainsi qu'un vocabulaire que nous avons créé nous même, et renvoie la matrice d'embeddings tiré du modèle chargé, pour les mots notre vocabulaire et dans le bon ordre.\n",
    "Remarque: les mots inconnus sont représentés par le vecteur nul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:34.772048Z",
     "start_time": "2020-03-10T18:36:34.682256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5002, 300)\n"
     ]
    }
   ],
   "source": [
    "print(GloveEmbeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:35.535379Z",
     "start_time": "2020-03-10T18:36:34.778998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________ GLOVE ________________________\n",
      "\n",
      "VOCABULAIRE 5000 - voisin de 'good'\n",
      "\n",
      "     distance 'euclidean': ['better', 'well', 'always', 'really', 'sure', 'way', 'so', 'but', 'excellent']\n",
      "     distance 'cosine'   : ['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']\n"
     ]
    }
   ],
   "source": [
    "print(\"________________________ GLOVE ________________________\\n\")  \n",
    "\n",
    "print('VOCABULAIRE 5000 - voisin de \\'good\\'\\n')\n",
    "\n",
    "print(\"     distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good'))\n",
    "print(\"     distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "Les résultats obtenus avec ce Glove Embedding sont encore différents que les méthodes précédentes et pas spécialement meilleur puisqu'on trouve des mots comme 'always', 'you', 'things', 'think', 'sure', 'way' qui n'ont pas spécialement de lien sémantique avec le mot 'good'. Bizarrement, malgré le fait que Gl0ve soit pré-appris (sur des textes plus variés), les voisins de 'good' ne sont pas spécialement meilleurs (pas de synonyme) , peut-être que les critiques de film n'ont pas beaucoup de synonymes de \"good\" dans leur vocabulaire ...\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "\n",
    "#### Le modèle skip-gram\n",
    "\n",
    "Le modèle skip-gram de base estime les probabilités d'une paire de mots $(i, j)$ d'apparaître ensemble:\n",
    "\n",
    "\n",
    "$$P(j \\mid i) = \\frac{\\exp(w_{i} c_{j})}{\\sum_{j'\\in V}\\exp(w_{i} c_{j'})}$$\n",
    "\n",
    "\n",
    "où $w_{i}$ est le vecteur ligne (du mot) $i$ et $c_{j}$ est le vecteur colonne (d'un mot du contexte) $j$. L'objectif est de minimiser la quantité suivante: \n",
    "\n",
    "\n",
    "$$ -\\sum_{i=1}^{m} \\sum_{k=1}^{|V|} \\textbf{1}\\{o_{i}=k\\} \\log \\frac{\\exp(w_{i} c_{k})}{\\sum_{j=1}^{|V|} \\exp(w_{i} c_{j})}$$\n",
    "\n",
    "\n",
    "ou $V$ est le vocabulaire.\n",
    "Les entrées $w_{i}$ sont les représentations des mots, que l'on met à jour pendant l'entraînement, et la sortie est un vecteur *one-hot* $o$, qui ne contient qu'un seul $1$ et des $0$. Par exemple, si `good` est le  47ème mot du vocabulaire, la sortie $o$ pour un exemple ou `good` est le mot à prédire consistera en des $0$s partout sauf un $1$ en 47ème position du vecteur. `good` sera le mot à prédire lorsque l'entrée $w$ sera un mot de son contexte\n",
    "On obient donc cette sortie avec softmax standard - on ajoute un terme de biais $b$.\n",
    "\n",
    "\n",
    "$$ o = \\textbf{softmax}(w_{i}C + b)$$\n",
    "\n",
    "\n",
    "Si l'on utilise l'ensemble des représentations pour tout le vocabulaire (la matrice $W$) comme entrée, on obtient \n",
    "\n",
    "\n",
    "$$ O = \\textbf{softmax}(WC + b)$$\n",
    "\n",
    "\n",
    "et on revient ainsi à l'idée centrale de toutes nos méthodes: on cherche à obtenir des représentations de mots à partir de comptes de co-occurences. Ici, on entraîne les paramètres contenus dans $W$ et $C$, deux matrices représentants les mots en dimension réduite (300) de façon à ce que leur produit scalaire soit le plus proche possible des co-occurences observées dans les données, à l'aide d'un objectif de maximum de vraisemblance.\n",
    "\n",
    "#### Le skip gram avec negative sampling\n",
    "\n",
    "L'entraînement du modèle skip-gram implique de calculer une somme sur l'ensemble du vocabulaire, à cause du **softmax**. Dès que la taille du vocabulaire augmente, cela devient infaisalbe. Afin de rendre les calculs plus rapides, on change l'objectif et on utilise la méthode du *negative sampling* (ou, celle, très proche, du *noise contrastive estimation*).\n",
    "\n",
    "\n",
    "Si on note $\\mathcal{D}$ l'ensemble des données et que l'on not $\\mathcal{D}'$ un ensemble de paires de mots qui ne sont **pas** dans les données (et qu'en pratique, l'on tire aléatoirement, l'objectif est:\n",
    "\n",
    "\n",
    "$$\\sum_{i, j \\in \\mathcal{D}}-\\log\\sigma(w_{i}c_{j}) + \\sum_{i, j \\in \\mathcal{D}'}\\log\\sigma(w_{i}c_{j})$$\n",
    "\n",
    "\n",
    "ou $\\sigma$ est la fonction d'activation sigmoide $\\frac{1}{1 + \\exp(-x)}$.\n",
    "Une pratique commune est de générer les paires de $\\mathcal{D}'$ de manière proportionelle aux fréquences des mots dans les données d'entraînement (ce qu'on appelle la distribution unigramme):\n",
    "\n",
    "\n",
    "$$P(w) = \\frac{\\textbf{T}(w)^{0.75}}{\\sum_{w'\\in V} \\textbf{T}(w')}$$\n",
    "\n",
    "\n",
    "Bien que différente, cette nouvelle fonction objectif est une approximation suffisante de la précédente, et est basée sur le même principe. De nombreuses recherches ont été effectuées sur cet objectif: par exemple, [Levy and Golberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) démontre que l'objectif calcule la matrice de PMI décalé d'une valeur constante. On peut aussi voir [Cotterell et al. 2017](https://aclanthology.coli.uni-saarland.de/papers/E17-2028/e17-2028) pour une interprétation de l'algorithme comme une variante de la PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser la bibliothèque ```gensim``` pour son implémentation de word2vec en python. On va devoir en faire une utilisation un peu spécifique, puisqu'on veut conserver le même vocabulaire qu'auparavant: on va d'abord créer la classe, puis récupérer le vocabulaire qu'on a utilisé plus haut. \n",
    "Pour ne pas à avoir à mettre toutes les données en mémoire d'un coup, on définit un générateur, qui prendra toutes les données en entrée et les pré-traitera et renverra à la classe ```Word2Vec``` phrase par phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:38.877267Z",
     "start_time": "2020-03-10T18:36:35.541137Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_5 = Word2Vec(size=300, window=5, null_word=1, iter=30)\n",
    "model_5.build_vocab_from_freq(word_counts_5k)\n",
    "\n",
    "model_20 = Word2Vec(size=300, window=20, null_word=1, iter=30)\n",
    "model_20.build_vocab_from_freq(word_counts_5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:38.883938Z",
     "start_time": "2020-03-10T18:36:38.880035Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_generator(large_corpus):\n",
    "    for line in large_corpus:\n",
    "        yield clean_and_tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:41.450821Z",
     "start_time": "2020-03-10T18:36:38.887842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851105, 1305272)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:44.013878Z",
     "start_time": "2020-03-10T18:36:41.452773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850944, 1305272)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_20.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:44.022585Z",
     "start_time": "2020-03-10T18:36:44.015752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5002, 300)\n",
      "(5002, 300)\n"
     ]
    }
   ],
   "source": [
    "W2VEmbeddings_5  = model_5.wv.vectors\n",
    "W2VEmbeddings_20 = model_20.wv.vectors\n",
    "print(W2VEmbeddings_5.shape)\n",
    "print(W2VEmbeddings_20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:45.482682Z",
     "start_time": "2020-03-10T18:36:44.025562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________ W2V ________________________\n",
      "\n",
      "VOCABULAIRE 5000 - voisin de 'good'\n",
      "\n",
      "    [5 mots] : \n",
      "         distance 'euclidean': ['well', 'funny', 'very', 'bad', 'quite', 'fun', 'pretty', 'great', 'certainly']\n",
      "         distance 'cosine'   : ['funny', 'well', 'pretty', 'bad', 'very', 'quite', 'great', 'fun', 'lot']\n",
      "\n",
      "    [20 mots] : \n",
      "         distance 'euclidean': ['very', 'great', 'acting', 'well', 'although', 'comedy', 'bad', 'funny', 'though']\n",
      "         distance 'cosine'   : ['very', 'great', 'acting', 'although', 'here', 'well', 'comedy', 'bad', 'its']\n"
     ]
    }
   ],
   "source": [
    "print(\"________________________ W2V ________________________\\n\")  \n",
    "\n",
    "print('VOCABULAIRE 5000 - voisin de \\'good\\'\\n')\n",
    "\n",
    "print('    [5 mots] : ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, W2VEmbeddings_5, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, W2VEmbeddings_5, 'good'))\n",
    "print(\"\")\n",
    "print('    [20 mots] : ')\n",
    "print(\"         distance \\'euclidean\\':\", get_neighbors(euclidean, vocab_5k, W2VEmbeddings_20, 'good'))\n",
    "print(\"         distance \\'cosine\\'   :\", get_neighbors(cosine, vocab_5k, W2VEmbeddings_20, 'good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application à l'analyse de sentiments\n",
    "\n",
    "On va maintenant utiliser ces représentations pour l'analyse de sentiments. \n",
    "Le modèle de base, comme hier, sera construit en deux étapes:\n",
    "- Une fonction permettant d'obtenir des représentations vectorielles des critiques, à partir des textes, du vocabulaire, et des représentations vectorielles des mots. Une telle fonction (à compléter ci-dessous) va associer à chaque mot d'une critique son embeddings, et créer la représentation pour l'ensemble de la phrase en sommant ces embeddings.\n",
    "- Un classifieur qui prendra ces représentations en entrée et réalisera une prédiction. Pour le réaliser, on pourra utiliser d'abord la régression logistique ```LogisticRegression``` de ```scikit-learn```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:36:45.510986Z",
     "start_time": "2020-03-10T18:36:45.489515Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n",
    "    \"\"\"\n",
    "    Represent the sentences as a combination of the vector of its words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : a list of sentences   \n",
    "    vocabulary : dict - From words to indexes of vector.\n",
    "    embeddings : Matrix containing word representations\n",
    "    np_func : function (default: np.sum)-  A numpy matrix operation that can be applied columnwise, \n",
    "                                            like `np.mean`, `np.sum`, or `np.prod`. \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
    "    \"\"\"\n",
    "    representations = np.zeros((len(texts),embeddings.shape[1]))\n",
    "    \n",
    "    for i,text in enumerate(texts):\n",
    "        \n",
    "        text          = clean_and_tokenize(text)\n",
    "        word_index    = [vocabulary[word] if word in vocabulary else vocabulary['UNK'] for word in text] # index of the word in the vocabulary\n",
    "        word_embedded = embeddings[word_index,:] # map the word in embedded representation\n",
    "        text_embedded = np_func(word_embedded,axis=0) # apply input function\n",
    "        \n",
    "        representations[i,:] = text_embedded\n",
    "        \n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:43:30.733157Z",
     "start_time": "2020-03-10T18:36:45.516844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Co-occurences =========\n",
      "test score : 0.692\n",
      "Score de classification: 0.698 (std 0.010)\n",
      "\n",
      "========= PPMI =========\n",
      "test score : 0.818\n",
      "Score de classification: 0.830 (std 0.009)\n",
      "\n",
      "======== TF-IDF =========\n",
      "test score : 0.79\n",
      "Score de classification: 0.807 (std 0.015)\n",
      "\n",
      "========== Glove ========\n",
      "test score : 0.753\n",
      "Score de classification: 0.789 (std 0.017)\n",
      "\n",
      "========= Word2Vec ========\n",
      "test score : 0.729\n",
      "Score de classification: 0.756 (std 0.023)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "print(\"======== Co-occurences =========\") \n",
    "rep = sentence_representations(texts, vocab_5k, M5dist)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000,tol=0.001).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n========= PPMI =========\")\n",
    "rep = sentence_representations(texts, vocab_5k, PPMI5dist)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n======== TF-IDF =========\") \n",
    "rep = sentence_representations(texts, vocab_5k, TFIDF5dist)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n========== Glove ========\")\n",
    "rep = sentence_representations(texts, vocab_5k, GloveEmbeddings)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n========= Word2Vec ========\")\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_5)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant comparer l'ensemble des méthodes, et notamment répondre aux questions suivantes:\n",
    "\n",
    "- Pourquoi peut-on s'attendre à ce que les résultats obtenus avec les embeddings extraits des représentations pré-apprises avec Gl0ve soient bien meilleurs que les autres ? Quel serait le moyen de comparer de manière 'juste' Gl0ve avec les autres méthodes d'apprentissage de représentations ?\n",
    "\n",
    "<font color=\"blue\"> \n",
    "On peut s'attendre à de meilleurs résultats avec Gl0ve puisque le modèle a été pré-appris sur des textes plus variés et donc plus représentatif du langage, contrairement à notre autres modèles qui apprennent seulement sur des critiques de films. Pour comparer, il faudrait que le modèle Gl0ve apprenne sa représentation sur le même ensemble de donnnées (ici, des critiques de film) que les autres modèles. \n",
    "    \n",
    "Il n'empêche que pour la classification de critique de film Gl0ve a de moins résultats que les autres modèles. En fait, les autres modèles ont une représentation spécifique aux critiques de film (pas du langage en générale) et donc classent mieux les critiques de film. Mais si on testait les modèles sur d'autres corpus, Gl0ve devrait mieux classer.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quelle matrice permet d'obtenir les meilleures représentations via SVD ? (Co-occurences, Tf-Idf, PPMI ..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**avec Reduction de Dimension SVD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:43:45.358002Z",
     "start_time": "2020-03-10T18:43:30.734999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voisin de 'good' [5 mots] - [pondéré] distance 'cosine' :\n",
      " - Co-occurence : ['bad', 'very', 'funny', 'fine', 'great', 'plot', 'pace', 'shock', 'basically']\n",
      " - PPMI         : ['but', 'bad', 'this', 'very', 'some', 'it', 'movie', 'is', 'a']\n",
      " - TFIDF        : ['moments', 'supposed', 'very', 'it', 'that', 'too', 'movie', 'plausible', 'but']\n"
     ]
    }
   ],
   "source": [
    "print('voisin de \\'good\\' [5 mots] - [pondéré] distance \\'cosine\\' :')\n",
    "SVD_M5dist = svd.fit_transform(M5dist)\n",
    "print(\" - Co-occurence :\", get_neighbors(cosine, vocab_5k, SVD_M5dist, 'good'))\n",
    "SVD_PPMI5dist = svd.fit_transform(PPMI5dist)\n",
    "print(\" - PPMI         :\", get_neighbors(cosine, vocab_5k, SVD_PPMI5dist, 'good'))\n",
    "SVD_TFIDF5dist = svd.fit_transform(TFIDF5dist)\n",
    "print(\" - TFIDF        :\", get_neighbors(cosine, vocab_5k, SVD_TFIDF5dist, 'good'))\n",
    "\n",
    "# W2V et Gl0ve sont deja de dimension 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T18:44:29.313603Z",
     "start_time": "2020-03-10T18:43:45.360378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Co-occurences =========\n",
      "test score : 0.683\n",
      "Score de classification: 0.690 (std 0.014)\n",
      "\n",
      "========= PPMI =========\n",
      "test score : 0.768\n",
      "Score de classification: 0.790 (std 0.015)\n",
      "\n",
      "======== TF-IDF =========\n",
      "test score : 0.715\n",
      "Score de classification: 0.739 (std 0.021)\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Co-occurences =========\") \n",
    "rep = sentence_representations(texts, vocab_5k, SVD_M5dist)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, tol=0.001).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n========= PPMI =========\")\n",
    "rep = sentence_representations(texts, vocab_5k, SVD_PPMI5dist)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n======== TF-IDF =========\") \n",
    "rep = sentence_representations(texts, vocab_5k, SVD_TFIDF5dist)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> \n",
    "Après SVD (300 features), les scores de classification ont diminué mais c'est toujours le modèle PPMI qui obtient un meilleur score (que Co-occurences et TF-IDF). Mais on peut maintenant faire une comparaison plus 'juste' avec les modèle W2V et Glove puisqu'on a maintenant le même nombre de features, et on voit maintenant que W2V et Gl0ve ne s'en sorte pas si mal (ils sont maintenant mieux que TF-IDF)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec est difficile à paramétrer. Essayez d'améliorer les représentations en changeant la taille du contexte, le nombre d'itérations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T19:24:14.961237Z",
     "start_time": "2020-03-10T19:13:26.947650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____ contexte = 20 mots _______\n",
      "\n",
      "test score : 0.734\n",
      "Score de classification: 0.735 (std 0.012)\n",
      "\n",
      "______ sg = 1 (skipgram) ______\n",
      "\n",
      "test score : 0.735\n",
      "Score de classification: 0.757 (std 0.009)\n",
      "\n",
      "_______ negative = 10 _________\n",
      "\n",
      "test score : 0.733\n",
      "Score de classification: 0.734 (std 0.013)\n",
      "\n",
      "_______ ns_exponent = 0.5 _______\n",
      "\n",
      "test score : 0.723\n",
      "Score de classification: 0.731 (std 0.023)\n",
      "\n",
      "______ min_count = 10 _______\n",
      "\n",
      "test score : 0.724\n",
      "Score de classification: 0.744 (std 0.020)\n",
      "\n",
      "_________ size = 1000 __________\n",
      "\n",
      "test score : 0.728\n",
      "Score de classification: 0.731 (std 0.018)\n",
      "\n",
      "______ iter = 50 _______\n",
      "\n",
      "test score : 0.731\n",
      "Score de classification: 0.746 (std 0.016)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n_____ contexte = 20 mots _______\\n\")\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_20)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n______ sg = 1 (skipgram) ______\\n\")\n",
    "model_sg = Word2Vec(size=300, window=20, null_word=1, iter=30, sg=1)\n",
    "model_sg.build_vocab_from_freq(word_counts_5k)\n",
    "model_sg.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)\n",
    "W2VEmbeddings_sg = model_sg.wv.vectors\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_sg)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n_______ negative = 10 _________\\n\")\n",
    "model_n = Word2Vec(size=300, window=20, null_word=1, iter=30, negative=10)\n",
    "model_n.build_vocab_from_freq(word_counts_5k)\n",
    "model_n.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)\n",
    "W2VEmbeddings_n = model_n.wv.vectors\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_n)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n_______ ns_exponent = 0.5 _______\\n\")\n",
    "model_exp = Word2Vec(size=300, window=20, null_word=1, iter=30, ns_exponent=0.5)\n",
    "model_exp.build_vocab_from_freq(word_counts_5k)\n",
    "model_exp.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)\n",
    "W2VEmbeddings_exp = model_exp.wv.vectors\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_exp)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n______ min_count = 10 _______\\n\")\n",
    "model_min = Word2Vec(size=300, window=20, null_word=1, iter=30, min_count=10)\n",
    "model_min.build_vocab_from_freq(word_counts_5k)\n",
    "model_min.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)\n",
    "W2VEmbeddings_min = model_min.wv.vectors\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_min)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n_________ size = 1000 __________\\n\")\n",
    "model_size = Word2Vec(size=1000, window=20, null_word=1, iter=30)\n",
    "model_size.build_vocab_from_freq(word_counts_5k)\n",
    "model_size.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)\n",
    "W2VEmbeddings_size = model_size.wv.vectors\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_size)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "print(\"\\n______ iter = 50 _______\\n\")\n",
    "model_min = Word2Vec(size=300, window=20, null_word=1, iter=50)\n",
    "model_min.build_vocab_from_freq(word_counts_5k)\n",
    "model_min.train(preprocess_generator(texts[:]), total_examples=len(texts), epochs=30, report_delay=1)\n",
    "W2VEmbeddings_min = model_min.wv.vectors\n",
    "rep = sentence_representations(texts, vocab_5k, W2VEmbeddings_min)\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=10000).fit(rep[::2], y[::2])\n",
    "print('test score :', clf.score(rep[1::2], y[1::2]))\n",
    "scores = cross_val_score(clf, rep, y, cv=5)\n",
    "print('Score de classification: %.3f (std %.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T17:07:23.024128Z",
     "start_time": "2020-03-10T17:07:23.014368Z"
    }
   },
   "source": [
    "<font color=\"blue\"> \n",
    "En testant d'autres paramètres, j'obtiens peu de changement. Le mode skipgram semble donner des résultats légèrement mieux que le mode CBOW (0.760 contre 0.756)\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
